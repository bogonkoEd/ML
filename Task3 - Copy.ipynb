{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Country Name                | Country Code   | Indicator Name    | Indicator Code   | 1960        | 1961        | 1962        | 1963        | 1964        | 1965        | 1966        | 1967        | 1968        | 1969        | 1970        | 1971        | 1972        | 1973        | 1974        | 1975        | 1976        | 1977        | 1978        | 1979        | 1980        | 1981        | 1982        | 1983        | 1984        | 1985        | 1986        | 1987        | 1988        | 1989        | 1990        | 1991        | 1992        | 1993        | 1994        | 1995        | 1996        | 1997        | 1998        | 1999        | 2000        | 2001        | 2002        | 2003        | 2004        | 2005        | 2006        | 2007        | 2008        | 2009        | 2010        | 2011        | 2012        | 2013        | 2014        | 2015        | 2016        | 2017        | 2018        | 2019        | 2020        | 2021        | 2022        | 2023   | Unnamed: 68   |\n",
      "|:----------------------------|:---------------|:------------------|:-----------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:------------|:-------|:--------------|\n",
      "| Aruba                       | ABW            | Population, total | SP.POP.TOTL      | 54608       | 55811       | 56682       | 57475       | 58178       | 58782       | 59291       | 59522       | 59471       | 59330       | 59106       | 58816       | 58855       | 59365       | 60028       | 60715       | 61193       | 61465       | 61738       | 62006       | 62267       | 62614       | 63116       | 63683       | 64174       | 64478       | 64553       | 64450       | 64332       | 64596       | 65712       | 67864       | 70192       | 72360       | 74710       | 77050       | 79417       | 81858       | 84355       | 86867       | 89101       | 90691       | 91781       | 92701       | 93540       | 94483       | 95606       | 96787       | 97996       | 99212       | 100341      | 101288      | 102112      | 102880      | 103594      | 104257      | 104874      | 105439      | 105962      | 106442      | 106585      | 106537      | 106445      | nan    | nan           |\n",
      "| Africa Eastern and Southern | AFE            | Population, total | SP.POP.TOTL      | 1.30693e+08 | 1.34169e+08 | 1.37836e+08 | 1.41631e+08 | 1.45606e+08 | 1.49742e+08 | 1.53956e+08 | 1.58313e+08 | 1.62875e+08 | 1.67596e+08 | 1.72476e+08 | 1.77503e+08 | 1.82599e+08 | 1.87902e+08 | 1.93513e+08 | 1.99284e+08 | 2.05203e+08 | 2.11121e+08 | 2.17481e+08 | 2.24316e+08 | 2.30968e+08 | 2.37937e+08 | 2.45387e+08 | 2.5278e+08  | 2.60209e+08 | 2.67938e+08 | 2.76036e+08 | 2.8449e+08  | 2.92795e+08 | 3.01125e+08 | 3.09891e+08 | 3.18544e+08 | 3.26934e+08 | 3.35625e+08 | 3.44418e+08 | 3.53467e+08 | 3.62986e+08 | 3.72352e+08 | 3.81716e+08 | 3.91486e+08 | 4.01601e+08 | 4.12002e+08 | 4.22741e+08 | 4.33807e+08 | 4.45282e+08 | 4.57154e+08 | 4.69509e+08 | 4.82406e+08 | 4.95749e+08 | 5.0941e+08  | 5.2346e+08  | 5.37793e+08 | 5.52531e+08 | 5.67892e+08 | 5.83651e+08 | 6.00008e+08 | 6.16378e+08 | 6.32747e+08 | 6.49757e+08 | 6.67243e+08 | 6.85113e+08 | 7.02977e+08 | 7.20859e+08 | nan    | nan           |\n",
      "| Afghanistan                 | AFG            | Population, total | SP.POP.TOTL      | 8.62247e+06 | 8.79014e+06 | 8.96905e+06 | 9.15746e+06 | 9.35551e+06 | 9.56515e+06 | 9.78315e+06 | 1.001e+07   | 1.02478e+07 | 1.04945e+07 | 1.0753e+07  | 1.10159e+07 | 1.12868e+07 | 1.15753e+07 | 1.18699e+07 | 1.21574e+07 | 1.24253e+07 | 1.26873e+07 | 1.29389e+07 | 1.29864e+07 | 1.24866e+07 | 1.11552e+07 | 1.00883e+07 | 9.95145e+06 | 1.02437e+07 | 1.05122e+07 | 1.04484e+07 | 1.03228e+07 | 1.03835e+07 | 1.06732e+07 | 1.06948e+07 | 1.07452e+07 | 1.20574e+07 | 1.40038e+07 | 1.54556e+07 | 1.64189e+07 | 1.71066e+07 | 1.77888e+07 | 1.84931e+07 | 1.92628e+07 | 1.9543e+07  | 1.96886e+07 | 2.10003e+07 | 2.26451e+07 | 2.35536e+07 | 2.44112e+07 | 2.54429e+07 | 2.59033e+07 | 2.64272e+07 | 2.73853e+07 | 2.81897e+07 | 2.92492e+07 | 3.04665e+07 | 3.15412e+07 | 3.27162e+07 | 3.37535e+07 | 3.46362e+07 | 3.56434e+07 | 3.66868e+07 | 3.77695e+07 | 3.89722e+07 | 4.00995e+07 | 4.11288e+07 | nan    | nan           |\n",
      "| Africa Western and Central  | AFW            | Population, total | SP.POP.TOTL      | 9.72563e+07 | 9.9314e+07  | 1.01445e+08 | 1.03668e+08 | 1.0596e+08  | 1.08336e+08 | 1.10798e+08 | 1.1332e+08  | 1.15922e+08 | 1.18616e+08 | 1.21425e+08 | 1.24336e+08 | 1.27364e+08 | 1.30563e+08 | 1.33954e+08 | 1.37549e+08 | 1.41258e+08 | 1.45123e+08 | 1.49207e+08 | 1.5346e+08  | 1.57826e+08 | 1.62323e+08 | 1.67023e+08 | 1.71567e+08 | 1.76054e+08 | 1.80817e+08 | 1.8572e+08  | 1.9076e+08  | 1.9597e+08  | 2.01392e+08 | 2.06739e+08 | 2.12173e+08 | 2.17966e+08 | 2.23789e+08 | 2.29676e+08 | 2.35861e+08 | 2.422e+08   | 2.48713e+08 | 2.55483e+08 | 2.62397e+08 | 2.69612e+08 | 2.7716e+08  | 2.84952e+08 | 2.92978e+08 | 3.01265e+08 | 3.09825e+08 | 3.18601e+08 | 3.27613e+08 | 3.36894e+08 | 3.46475e+08 | 3.56338e+08 | 3.66489e+08 | 3.76798e+08 | 3.87205e+08 | 3.97856e+08 | 4.0869e+08  | 4.19778e+08 | 4.31139e+08 | 4.42647e+08 | 4.54306e+08 | 4.66189e+08 | 4.78186e+08 | 4.90331e+08 | nan    | nan           |\n",
      "| Angola                      | AGO            | Population, total | SP.POP.TOTL      | 5.3572e+06  | 5.44133e+06 | 5.5214e+06  | 5.59983e+06 | 5.6732e+06  | 5.73658e+06 | 5.78704e+06 | 5.8275e+06  | 5.8682e+06  | 5.92839e+06 | 6.0297e+06  | 6.17705e+06 | 6.36473e+06 | 6.57823e+06 | 6.80249e+06 | 7.03271e+06 | 7.26678e+06 | 7.5119e+06  | 7.77159e+06 | 8.04322e+06 | 8.33005e+06 | 8.63146e+06 | 8.94715e+06 | 9.27671e+06 | 9.6177e+06  | 9.97062e+06 | 1.03326e+07 | 1.06941e+07 | 1.10603e+07 | 1.14395e+07 | 1.18286e+07 | 1.22287e+07 | 1.26325e+07 | 1.30383e+07 | 1.3462e+07  | 1.39123e+07 | 1.43834e+07 | 1.48711e+07 | 1.53669e+07 | 1.58708e+07 | 1.63941e+07 | 1.69416e+07 | 1.75161e+07 | 1.81243e+07 | 1.87711e+07 | 1.9451e+07  | 2.01623e+07 | 2.09097e+07 | 2.16915e+07 | 2.25077e+07 | 2.33642e+07 | 2.42591e+07 | 2.51883e+07 | 2.6147e+07  | 2.71283e+07 | 2.81277e+07 | 2.91547e+07 | 3.02086e+07 | 3.12735e+07 | 3.23536e+07 | 3.34285e+07 | 3.45038e+07 | 3.5589e+07  | nan    | nan           |\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266 entries, 0 to 265\n",
      "Data columns (total 69 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Country Name    266 non-null    object \n",
      " 1   Country Code    266 non-null    object \n",
      " 2   Indicator Name  266 non-null    object \n",
      " 3   Indicator Code  266 non-null    object \n",
      " 4   1960            264 non-null    float64\n",
      " 5   1961            264 non-null    float64\n",
      " 6   1962            264 non-null    float64\n",
      " 7   1963            264 non-null    float64\n",
      " 8   1964            264 non-null    float64\n",
      " 9   1965            264 non-null    float64\n",
      " 10  1966            264 non-null    float64\n",
      " 11  1967            264 non-null    float64\n",
      " 12  1968            264 non-null    float64\n",
      " 13  1969            264 non-null    float64\n",
      " 14  1970            264 non-null    float64\n",
      " 15  1971            264 non-null    float64\n",
      " 16  1972            264 non-null    float64\n",
      " 17  1973            264 non-null    float64\n",
      " 18  1974            264 non-null    float64\n",
      " 19  1975            264 non-null    float64\n",
      " 20  1976            264 non-null    float64\n",
      " 21  1977            264 non-null    float64\n",
      " 22  1978            264 non-null    float64\n",
      " 23  1979            264 non-null    float64\n",
      " 24  1980            264 non-null    float64\n",
      " 25  1981            264 non-null    float64\n",
      " 26  1982            264 non-null    float64\n",
      " 27  1983            264 non-null    float64\n",
      " 28  1984            264 non-null    float64\n",
      " 29  1985            264 non-null    float64\n",
      " 30  1986            264 non-null    float64\n",
      " 31  1987            264 non-null    float64\n",
      " 32  1988            264 non-null    float64\n",
      " 33  1989            264 non-null    float64\n",
      " 34  1990            265 non-null    float64\n",
      " 35  1991            265 non-null    float64\n",
      " 36  1992            265 non-null    float64\n",
      " 37  1993            265 non-null    float64\n",
      " 38  1994            265 non-null    float64\n",
      " 39  1995            265 non-null    float64\n",
      " 40  1996            265 non-null    float64\n",
      " 41  1997            265 non-null    float64\n",
      " 42  1998            265 non-null    float64\n",
      " 43  1999            265 non-null    float64\n",
      " 44  2000            265 non-null    float64\n",
      " 45  2001            265 non-null    float64\n",
      " 46  2002            265 non-null    float64\n",
      " 47  2003            265 non-null    float64\n",
      " 48  2004            265 non-null    float64\n",
      " 49  2005            265 non-null    float64\n",
      " 50  2006            265 non-null    float64\n",
      " 51  2007            265 non-null    float64\n",
      " 52  2008            265 non-null    float64\n",
      " 53  2009            265 non-null    float64\n",
      " 54  2010            265 non-null    float64\n",
      " 55  2011            265 non-null    float64\n",
      " 56  2012            265 non-null    float64\n",
      " 57  2013            265 non-null    float64\n",
      " 58  2014            265 non-null    float64\n",
      " 59  2015            265 non-null    float64\n",
      " 60  2016            265 non-null    float64\n",
      " 61  2017            265 non-null    float64\n",
      " 62  2018            265 non-null    float64\n",
      " 63  2019            265 non-null    float64\n",
      " 64  2020            265 non-null    float64\n",
      " 65  2021            265 non-null    float64\n",
      " 66  2022            265 non-null    float64\n",
      " 67  2023            0 non-null      float64\n",
      " 68  Unnamed: 68     0 non-null      float64\n",
      "dtypes: float64(65), object(4)\n",
      "memory usage: 143.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv(\"Lab 3 Dataset.csv\", skiprows=4)\n",
    "\n",
    "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Population\n",
      "0  1961   5441333.0\n",
      "1  1962   5521400.0\n",
      "2  1963   5599827.0\n",
      "3  1964   5673199.0\n",
      "4  1965   5736582.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62 entries, 0 to 61\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Year        62 non-null     int64  \n",
      " 1   Population  62 non-null     float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 1.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "clean_df = df.drop(columns=['2023', 'Unnamed: 68'])\n",
    "clean_df = clean_df[clean_df['Indicator Name'] == 'Population, total'].copy()\n",
    "\n",
    "selected_country = 'Angola'\n",
    "df_country = clean_df[clean_df['Country Name'] == selected_country].copy()\n",
    "\n",
    "df_country_t = df_country.drop(columns=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code']).T\n",
    "df_country_t = df_country_t.iloc[1:].copy()\n",
    "df_country_t = df_country_t.reset_index()\n",
    "df_country_t.columns = ['Year', 'Population']\n",
    "df_country_t['Year'] = pd.to_numeric(df_country_t['Year'])\n",
    "df_country_t['Population'] = pd.to_numeric(df_country_t['Population'], errors='coerce')\n",
    "df_country_t.dropna(inplace=True)\n",
    "\n",
    "print(df_country_t.head())\n",
    "print(df_country_t.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 0.1634 - val_loss: 0.2865\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 0.1411 - val_loss: 0.2876\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - loss: 0.1206 - val_loss: 0.2888\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 322ms/step - loss: 0.1017 - val_loss: 0.2901\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 332ms/step - loss: 0.0846 - val_loss: 0.2915\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - loss: 0.0693 - val_loss: 0.2930\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - loss: 0.0558 - val_loss: 0.2946\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 348ms/step - loss: 0.0440 - val_loss: 0.2963\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 0.0340 - val_loss: 0.2979\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step - loss: 0.0256 - val_loss: 0.2996\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 388ms/step - loss: 0.0190 - val_loss: 0.3013\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 397ms/step - loss: 0.0140 - val_loss: 0.3030\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317ms/step - loss: 0.0103 - val_loss: 0.3047\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - loss: 0.0076 - val_loss: 0.3063\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 447ms/step - loss: 0.0060 - val_loss: 0.3078\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454ms/step - loss: 0.0052 - val_loss: 0.3093\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - loss: 0.0050 - val_loss: 0.3106\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 754ms/step - loss: 0.0054 - val_loss: 0.3118\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - loss: 0.0061 - val_loss: 0.3128\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - loss: 0.0070 - val_loss: 0.3136\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.0079 - val_loss: 0.3142\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - loss: 0.0089 - val_loss: 0.3147\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step - loss: 0.0096 - val_loss: 0.3150\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - loss: 0.0103 - val_loss: 0.3151\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - loss: 0.0107 - val_loss: 0.3151\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313ms/step - loss: 0.0108 - val_loss: 0.3149\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0108 - val_loss: 0.3146\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - loss: 0.0105 - val_loss: 0.3142\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 0.0101 - val_loss: 0.3137\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 356ms/step - loss: 0.0096 - val_loss: 0.3130\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 360ms/step - loss: 0.0090 - val_loss: 0.3124\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - loss: 0.0083 - val_loss: 0.3116\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 0.0076 - val_loss: 0.3109\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 361ms/step - loss: 0.0070 - val_loss: 0.3101\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436ms/step - loss: 0.0064 - val_loss: 0.3093\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - loss: 0.0059 - val_loss: 0.3085\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - loss: 0.0054 - val_loss: 0.3077\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step - loss: 0.0051 - val_loss: 0.3070\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 365ms/step - loss: 0.0049 - val_loss: 0.3063\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 0.0047 - val_loss: 0.3056\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 357ms/step - loss: 0.0047 - val_loss: 0.3050\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - loss: 0.0046 - val_loss: 0.3045\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 282ms/step - loss: 0.0047 - val_loss: 0.3040\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 0.0047 - val_loss: 0.3035\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 0.0048 - val_loss: 0.3031\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - loss: 0.0049 - val_loss: 0.3028\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - loss: 0.0050 - val_loss: 0.3025\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 321ms/step - loss: 0.0050 - val_loss: 0.3023\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - loss: 0.0050 - val_loss: 0.3021\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step - loss: 0.0050 - val_loss: 0.3019\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 353ms/step - loss: 0.0050 - val_loss: 0.3018\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - loss: 0.0050 - val_loss: 0.3018\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 0.0049 - val_loss: 0.3017\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0049 - val_loss: 0.3017\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 692ms/step - loss: 0.0048 - val_loss: 0.3017\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814ms/step - loss: 0.0047 - val_loss: 0.3018\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 427ms/step - loss: 0.0046 - val_loss: 0.3018\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step - loss: 0.0045 - val_loss: 0.3018\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 540ms/step - loss: 0.0045 - val_loss: 0.3019\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 329ms/step - loss: 0.0044 - val_loss: 0.3019\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - loss: 0.0043 - val_loss: 0.3020\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 563ms/step - loss: 0.0043 - val_loss: 0.3020\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609ms/step - loss: 0.0043 - val_loss: 0.3020\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446ms/step - loss: 0.0043 - val_loss: 0.3020\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step - loss: 0.0042 - val_loss: 0.3019\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - loss: 0.0042 - val_loss: 0.3019\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - loss: 0.0042 - val_loss: 0.3018\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - loss: 0.0042 - val_loss: 0.3017\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 0.0042 - val_loss: 0.3016\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.0042 - val_loss: 0.3015\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 342ms/step - loss: 0.0042 - val_loss: 0.3013\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - loss: 0.0042 - val_loss: 0.3011\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 688ms/step - loss: 0.0042 - val_loss: 0.3009\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 604ms/step - loss: 0.0041 - val_loss: 0.3007\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - loss: 0.0041 - val_loss: 0.3005\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657ms/step - loss: 0.0041 - val_loss: 0.3003\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - loss: 0.0040 - val_loss: 0.3001\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 786ms/step - loss: 0.0040 - val_loss: 0.2998\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 518ms/step - loss: 0.0040 - val_loss: 0.2996\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - loss: 0.0040 - val_loss: 0.2993\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - loss: 0.0039 - val_loss: 0.2991\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - loss: 0.0039 - val_loss: 0.2989\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - loss: 0.0039 - val_loss: 0.2986\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step - loss: 0.0039 - val_loss: 0.2984\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - loss: 0.0039 - val_loss: 0.2982\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - loss: 0.0038 - val_loss: 0.2980\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 340ms/step - loss: 0.0038 - val_loss: 0.2978\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 375ms/step - loss: 0.0038 - val_loss: 0.2976\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - loss: 0.0038 - val_loss: 0.2974\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - loss: 0.0038 - val_loss: 0.2972\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 278ms/step - loss: 0.0038 - val_loss: 0.2971\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step - loss: 0.0037 - val_loss: 0.2969\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step - loss: 0.0037 - val_loss: 0.2967\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - loss: 0.0037 - val_loss: 0.2966\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 351ms/step - loss: 0.0037 - val_loss: 0.2964\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - loss: 0.0037 - val_loss: 0.2963\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step - loss: 0.0036 - val_loss: 0.2962\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - loss: 0.0036 - val_loss: 0.2960\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - loss: 0.0036 - val_loss: 0.2959\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 438ms/step - loss: 0.0036 - val_loss: 0.2957\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 251ms/step\n",
      "MAE: 2.134187320214646\n",
      "RMSE: 2.2011100719393237\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df_country_t[['Year', 'Population']] = scaler.fit_transform(df_country_t[['Year', 'Population']])\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "sequence_length = 4  \n",
    "\n",
    "for i in range(len(df_country_t) - sequence_length):\n",
    "    X.append(df_country_t['Population'][i:i+sequence_length])\n",
    "    y.append(df_country_t['Population'][i+sequence_length])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  # Don't shuffle for time series\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(sequence_length,)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100, validation_split=0.4)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print('MAE:', mae)\n",
    "print('RMSE:', rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
